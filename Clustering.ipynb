{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0197558d-90b2-4cc3-b491-e9c02115c264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score, balanced_accuracy_score\n",
    "# %matplotlib ipympl\n",
    "# %matplotlib notebook\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, PredefinedSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "subject_id_test = 'S9'\n",
    "subject_id_val = 'S10'\n",
    "\n",
    "def Tuning_Params(model, tune_parameters, X_train, y_train, X_val=None, y_val=None, \n",
    "                  scoring_func='balanced_accuracy', cv=5, verbose=1):\n",
    "    if X_val is not None and y_val is not None: # specific validate set\n",
    "        val_fold = [1 for x in range(len(y_train))] + [0 for y in range(len(y_val))]\n",
    "        cv = PredefinedSplit(val_fold)\n",
    "        X_train_all = np.concatenate((X_train, X_val), axis=0)\n",
    "        y_train_all = np.concatenate((y_train, y_val), axis=0)\n",
    "    else:\n",
    "        X_train_all = X_train\n",
    "        y_train_all = y_train\n",
    "    # Cross validate to find best hyper parameters\n",
    "    clf = GridSearchCV(estimator=model, \n",
    "                       param_grid=tune_parameters, \n",
    "                       cv=cv, verbose=verbose, scoring=scoring_func,\n",
    "                       refit=False)\n",
    "    clf.fit(X_train_all, y_train_all)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4358f97d-1c43-4c89-88cc-177b74da141a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/nvtu/PhD_Work/StressDetection/DATA/MyDataset/WESAD'\n",
    "NAME_DATASET = 'WESAD'\n",
    "data_group = np.load(f'{DATA_DIR}/{NAME_DATASET}_WRIST_groups_1.npy')\n",
    "data_gt = np.load(f'{DATA_DIR}/{NAME_DATASET}_WRIST_ground_truth_1.npy')\n",
    "data_ft = np.load(f'{DATA_DIR}/{NAME_DATASET}_WRIST_stats_feats_1.npy')\n",
    "# data_ft_con = np.load(f'EmbededFt/{NAME_DATASET}/{NAME_DATASET}_WRIST_contrastive_embed_{subject_id_test}.npy')\n",
    "data_ft_con = np.load('Output/WESAD/EmbedFt/EmbedFt_Combine_Euclid_sample_cross_internal_S9.npy')\n",
    "\n",
    "# Create dataframe for dataset\n",
    "column_values = [f'f{x}' for x in range(data_ft.shape[1])]\n",
    "data_full_ori = pd.DataFrame(data = data_ft,  \n",
    "                         columns = column_values)\n",
    "data_full_ori['subject_id'] = data_group\n",
    "data_full_ori['label'] = data_gt\n",
    "\n",
    "data_full_con = pd.DataFrame(data = data_ft_con,  \n",
    "                         columns = column_values)\n",
    "data_full_con['subject_id'] = data_group\n",
    "data_full_con['label'] = data_gt\n",
    "\n",
    "list_subject_id = np.unique(data_full_ori['subject_id']).tolist()\n",
    "\n",
    "# data_train_val = data_full_ori[data_full_ori.subject_id != subject_id_test]\n",
    "# data_test = data_full_ori[data_full_ori.subject_id == subject_id_test]\n",
    "# # subject_id_validate = random.Random(1509).choices(list(set(data_train_val.subject_id)),k=1)[0]\n",
    "# # subject_id_validate = 'RY2'\n",
    "# # data_train = data_train_val[data_train_val.subject_id != subject_id_validate]\n",
    "# # data_validate = data_train_val[data_train_val.subject_id == subject_id_validate]\n",
    "# ft_names = data_full_ori.columns.tolist()\n",
    "\n",
    "# # Scaler Data\n",
    "# X_train_val = data_train_val.iloc[:,:-1].to_numpy()\n",
    "# y_train_val = data_train_val.iloc[:,-1].to_numpy()\n",
    "# X_test = data_test.iloc[:,:-1].to_numpy()\n",
    "# y_test = data_test.iloc[:,-1].to_numpy()\n",
    "\n",
    "# # scaler = RobustScaler()\n",
    "# # X_train[:,:-1] = scaler.fit_transform(X_train[:,:-1])\n",
    "# # X_validate[:,:-1] = scaler.transform(X_validate[:,:-1])\n",
    "# # X_test[:,:-1] = scaler.transform(X_test[:,:-1])\n",
    "# # joblib.dump(scaler, f'{SAVE_MODEL_DIR}/{NAME_DATASET}/Model/Scaler_LH_{subject_id_test}.joblib')\n",
    "\n",
    "# # Create Dataframe\n",
    "# df_train_ori = pd.DataFrame(data = X_train_val, columns = ft_names[:-1])\n",
    "# df_train_ori['label'] = y_train_val\n",
    "\n",
    "# df_test_ori = pd.DataFrame(data = X_test, columns = ft_names[:-1])\n",
    "# df_test_ori['label'] = y_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# data_train_val = data_full_con[data_full_con.subject_id != subject_id_test]\n",
    "# data_test = data_full_con[data_full_con.subject_id == subject_id_test]\n",
    "# # subject_id_validate = random.Random(1509).choices(list(set(data_train_val.subject_id)),k=1)[0]\n",
    "# # subject_id_validate = 'RY2'\n",
    "# # data_train = data_train_val[data_train_val.subject_id != subject_id_validate]\n",
    "# # data_validate = data_train_val[data_train_val.subject_id == subject_id_validate]\n",
    "# ft_names = data_full_con.columns.tolist()\n",
    "\n",
    "# # Scaler Data\n",
    "# X_train_val = data_train_val.iloc[:,:-1].to_numpy()\n",
    "# y_train_val = data_train_val.iloc[:,-1].to_numpy()\n",
    "# X_test = data_test.iloc[:,:-1].to_numpy()\n",
    "# y_test = data_test.iloc[:,-1].to_numpy()\n",
    "\n",
    "# # scaler = RobustScaler()\n",
    "# # X_train[:,:-1] = scaler.fit_transform(X_train[:,:-1])\n",
    "# # X_validate[:,:-1] = scaler.transform(X_validate[:,:-1])\n",
    "# # X_test[:,:-1] = scaler.transform(X_test[:,:-1])\n",
    "# # joblib.dump(scaler, f'{SAVE_MODEL_DIR}/{NAME_DATASET}/Model/Scaler_LH_{subject_id_test}.joblib')\n",
    "\n",
    "# # Create Dataframe\n",
    "# df_train_con = pd.DataFrame(data = X_train_val, columns = ft_names[:-1])\n",
    "# df_train_con['label'] = y_train_val\n",
    "\n",
    "# df_test_con = pd.DataFrame(data = X_test, columns = ft_names[:-1])\n",
    "# df_test_con['label'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f5e931e-8186-4a3b-b82e-ed65aacf38e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters by cross-validation for RF\n",
    "tuned_parameters = {'n_neighbors': [2, 5, 10, 15 ,20, 25, 50]}\n",
    "model = KNeighborsClassifier(n_jobs=None)\n",
    "\n",
    "list_acc_train = []\n",
    "list_acc_test = []\n",
    "list_f1_train = []\n",
    "list_f1_test = []\n",
    "list_bacc_train = []\n",
    "list_bacc_test = []\n",
    "\n",
    "data_test = data_full_con[data_full_con.subject_id == subject_id_test]\n",
    "data_validate = data_full_con[data_full_con.subject_id == subject_id_val]\n",
    "data_train = data_full_con[(data_full_con.subject_id != subject_id_test) & (data_full_ori.subject_id != subject_id_val)]\n",
    "\n",
    "# split test sets\n",
    "X_test = data_test.iloc[:,:-2].to_numpy()\n",
    "y_test = data_test.iloc[:,-1].to_numpy()\n",
    "\n",
    "# split into train - validate\n",
    "X_train = data_train.iloc[:,:-2].to_numpy()\n",
    "y_train = data_train.iloc[:,-1].to_numpy()\n",
    "\n",
    "X_validate = data_validate.iloc[:,:-2].to_numpy()\n",
    "y_validate = data_validate.iloc[:,-1].to_numpy()\n",
    "\n",
    "# validate_portion = 0.2\n",
    "# X_train, X_validate, y_train, y_validate = train_test_split(X_train_val, y_train_val, \n",
    "#                                                           test_size=validate_portion, \n",
    "#                                                           random_state=1509, stratify=y_train_val)\n",
    "\n",
    "# Scaler Data\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# # X_train = scaler.transform(X_train)\n",
    "# X_validate = scaler.transform(X_validate)\n",
    "# X_test = scaler.transform(X_test)\n",
    "X_train_val = np.concatenate((X_train, X_validate))\n",
    "y_train_val = np.concatenate((y_train, y_validate))\n",
    "\n",
    "# GridSearch\n",
    "gs_model = Tuning_Params(model=model, tune_parameters=tuned_parameters, X_train=X_train, y_train=y_train,\n",
    "                      X_val=X_validate, y_val=y_validate, scoring_func='balanced_accuracy', verbose=0)\n",
    "\n",
    "# Train again with best hyperparameter\n",
    "model_final = KNeighborsClassifier(n_jobs=None, n_neighbors=gs_model.best_params_['n_neighbors'])\n",
    "model_final.fit(X_train, y_train)\n",
    "\n",
    "# Prediction\n",
    "Y_pred_test = model_final.predict(X_test)\n",
    "Y_pred_train = model_final.predict(X_train_val)\n",
    "acc_test = accuracy_score(y_test, Y_pred_test)\n",
    "acc_train = accuracy_score(y_train_val, Y_pred_train)\n",
    "\n",
    "f1_test = f1_score(y_test, Y_pred_test)\n",
    "f1_train = f1_score(y_train_val, Y_pred_train)\n",
    "\n",
    "bacc_test = balanced_accuracy_score(y_test, Y_pred_test)\n",
    "bacc_train = balanced_accuracy_score(y_train_val, Y_pred_train)\n",
    "\n",
    "# append to list\n",
    "list_acc_train.append(acc_train)\n",
    "list_acc_test.append(acc_test)\n",
    "list_bacc_train.append(bacc_train)\n",
    "list_bacc_test.append(bacc_test)\n",
    "list_f1_train.append(f1_train)\n",
    "list_f1_test.append(f1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02ffe26c-b6d8-41e3-8117-8ce26bdaa07a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_model.best_params_['n_neighbors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e3bcb87-05a2-4b4e-b1bc-eaa044ba7e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9997914680648237, 0.9451882845188284)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d8893e7-5e62-4c41-a4b9-b36dc6e95d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9996342478633407, 0.8752380952380953)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bacc_train, bacc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4ce7bd5-dc25-438c-ab43-98612198aa3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9995418548334315, 0.8574537540805224)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_train, f1_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fa64c24-42db-44f8-8065-d8da9b0b8bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9941313155386082,\n",
       " 0.9451882845188284,\n",
       " 0.992324141826189,\n",
       " 0.8752380952380953,\n",
       " 0.9871334334791979,\n",
       " 0.8574537540805224)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_train, acc_test, bacc_train, bacc_test, f1_train, f1_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4d18d0-d6e2-468f-9a2c-d46dc6bbf213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create Dataframe\n",
    "# result = pd.DataFrame(data = list_subject_id,  \n",
    "#                       columns = ['subject_id'])\n",
    "# result['acc_train'] = list_acc_train\n",
    "# result['acc_test'] = list_acc_test\n",
    "# result['bacc_train'] = list_bacc_train\n",
    "# result['bacc_test'] = list_bacc_test\n",
    "# result['f1_train'] = list_f1_train\n",
    "# result['f1_test'] = list_f1_test\n",
    "\n",
    "# mean_result = result.iloc[:,1:].mean()\n",
    "# mean_result = mean_result.to_frame().T\n",
    "# mean_result['subject_id'] = 'Average'\n",
    "# result = pd.concat([result, mean_result], axis=0)\n",
    "\n",
    "# # Save to file\n",
    "# result.to_csv(f'Result/GENERIC_{NAME_DATASET}_SVM_balance_weight_contrastive_ft.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f98fd86-a607-48c1-aaa4-bf38c1e66037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506174df-0f42-478c-9e0d-f34f1f21d1ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679ff221-5f46-4845-81ba-6d2b2ab6c738",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6a43e8-de35-41c4-b118-3e72afb38bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c8e091-2f7d-498b-9817-99dc5c6994f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a32e6e10-a36e-4561-b284-38840c287749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cos_func = ContrastiveLoss_CosineSimilarity(margin=0.1, max_violation=True)\n",
    "# test_dataset = ContrastiveDataset(df=df_test_con, numb_samples=100000, k=1.5)\n",
    "# test_dataset.shuffle(seed=1509)\n",
    "# test_dataloader = make_ContrastiveDataLoader(test_dataset, batch_size=2048)\n",
    "# loss_total_test = calculate_contrastive_loss(test_dataloader, loss_func_con=cos_func)\n",
    "# loss_total_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e32084b7-27ac-4969-871e-be2df8551fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_ori: 0.27 - full_con: 0.21\n",
    "# test_ori: 0.33 - test_con: 0.22\n",
    "# train_ori: 0.27 - train_con: 0.21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc69ddc8-c129-4307-963e-c1d97085caf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to visualize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train_ori = df_train_ori.iloc[:,:-2].to_numpy()\n",
    "X_test_ori = df_test_ori.iloc[:,:-2].to_numpy()\n",
    "X_ori = np.concatenate((X_train_ori, X_test_ori))\n",
    "scaler = StandardScaler()\n",
    "X_ori = scaler.fit_transform(X_ori)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X_ori)\n",
    "X_ori_transform = pca.transform(X_ori)\n",
    "X_ori_train = X_ori_transform[:X_train_ori.shape[0],:]\n",
    "X_ori_test = X_ori_transform[X_train_ori.shape[0]:,:]\n",
    "y_train = df_train_ori.iloc[:,-1].to_numpy()\n",
    "y_test = df_test_ori.iloc[:,-1].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a34453d-78fb-46b9-88f3-462f2cf1ba6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f25511ad18e406b9e50df17e4afa202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_ori_train[:,0], X_ori_train[:,1], marker='o', s=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fefae2a-2495-4075-9b97-17ebd1a50dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_con = df_train_con.iloc[:,:-2].to_numpy()\n",
    "X_test_con = df_test_con.iloc[:,:-2].to_numpy()\n",
    "X_con = np.concatenate((X_train_con, X_test_con))\n",
    "scaler = StandardScaler()\n",
    "X_con = scaler.fit_transform(X_con)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X_con)\n",
    "X_con_transform = pca.transform(X_ori)\n",
    "X_con_train = X_con_transform[:X_train_con.shape[0],:]\n",
    "X_con_test = X_con_transform[X_train_con.shape[0]:,:]\n",
    "y_train = df_train_ori.iloc[:,-1].to_numpy()\n",
    "y_test = df_test_ori.iloc[:,-1].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45f8028f-f685-4a8b-9e35-2904bb91b23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_con_train[:,0], X_con_train[:,1], marker='o', s=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b896be1-c4e5-4bc9-97d7-a02a110d6cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d64a8996e5344b2fb687c1f30d3bbb87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(-10.0, 20.0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(X_con_train[:,0], X_con_train[:,1], X_con_train[:,2], marker='o', s=1)\n",
    "ax.scatter(X_con_test[:,0], X_con_test[:,1], X_con_test[:,2], marker='^', s=1)\n",
    "ax.set_xlim([-5, 5])\n",
    "ax.set_ylim([-5, 5])\n",
    "ax.set_zlim([-10, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5376862-e33c-4c37-9a36-cdbb12b7f59b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
