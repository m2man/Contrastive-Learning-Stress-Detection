{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "323c66a4-0d11-4e15-99f6-5a89464d24ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import kstest # lower mean higher similar\n",
    "from tqdm import tqdm\n",
    "NAME_DATASET = 'WESAD'\n",
    "SUBJECT_ID_TEST = 'S14'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "faafd453-6d1d-40fc-b744-5374f0af97a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/geekculture/techniques-to-measure-probability-distribution-similarity-9145678d68a6\n",
    "# smaller mean higher similarity\n",
    "# create the data distribution\n",
    "data_1 = abs(np.random.randn(1000))\n",
    "data_2 = np.random.lognormal(size=1000)\n",
    "#compute KL Divergence\n",
    "\"\"\"KL Divergence(P|Q)\"\"\"\n",
    "def KL_div(p_probs, q_probs):    \n",
    "    KL_div = p_probs * np.log(p_probs / q_probs)\n",
    "    return np.sum(KL_div)\n",
    "def JS_Div(p, q):\n",
    "    p = np.asarray(p)\n",
    "    q = np.asarray(q)\n",
    "    # normalize\n",
    "    p /= p.sum()\n",
    "    q /= q.sum()\n",
    "    m = (p + q) / 2\n",
    "    return (KL_div(p, m) + KL_div(q, m)) / 2\n",
    "# JS Divergence is symmetric\n",
    "result_JSD12= JS_Div(data_1, data_2)\n",
    "result_JSD21= JS_Div(data_2, data_1)\n",
    "\n",
    "def similar_distribution(dt1, dt2):\n",
    "    ncol = dt1.shape[-1]\n",
    "    sum_score = 0\n",
    "    for idx in range(ncol):\n",
    "        score = kstest(dt1[:,idx], dt2[:,idx])[0]\n",
    "        sum_score += score\n",
    "    mean_score = sum_score / ncol\n",
    "    return mean_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9e8fc8e-d0ee-41a1-840a-38b3d70071c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### READ DATASET #####\n",
    "if NAME_DATASET == 'WESAD':\n",
    "    DATA_DIR = '/home/nvtu/PhD_Work/StressDetection/DATA/MyDataset/WESAD'\n",
    "    data_group = np.load(f'{DATA_DIR}/{NAME_DATASET}_WRIST_groups_1_60.npy')\n",
    "    data_gt = np.load(f'{DATA_DIR}/{NAME_DATASET}_WRIST_ground_truth_1_60.npy')\n",
    "    data_ft = np.load(f'{DATA_DIR}/{NAME_DATASET}_WRIST_stats_feats_1_60.npy')\n",
    "else:\n",
    "    DATA_DIR = '/home/nvtu/PhD_Work/StressDetection/DATA/MyDataset/AffectiveROAD_Data/Database'\n",
    "    NAME_DATASET = 'AffectiveROAD'\n",
    "    data_group = np.load(f'{DATA_DIR}/{NAME_DATASET}_groups_1.npy')\n",
    "    data_gt = np.load(f'{DATA_DIR}/{NAME_DATASET}_ground_truth_1.npy')\n",
    "    data_ft = np.load(f'{DATA_DIR}/{NAME_DATASET}_stats_feats_1.npy')\n",
    "    indices = np.where(data_gt >= 0)[0]\n",
    "    data_ft = data_ft[indices]\n",
    "    data_group = data_group[indices]\n",
    "    data_gt = data_gt[indices]\n",
    "\n",
    "# Create dataframe for dataset\n",
    "column_values = [f'f{x}' for x in range(data_ft.shape[1])]\n",
    "data_full = pd.DataFrame(data = data_ft,  \n",
    "                         columns = column_values)\n",
    "data_full['subject_id'] = data_group\n",
    "data_full['label'] = data_gt\n",
    "list_subject_id = np.unique(data_full['subject_id']).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d00ac2b-539f-4fb7-94bd-4f24e1ecd6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 14/14 [00:55<00:00,  3.99s/it]\n"
     ]
    }
   ],
   "source": [
    "subject_id_test = SUBJECT_ID_TEST\n",
    "data_train_val = data_full[data_full.subject_id != subject_id_test]\n",
    "data_test = data_full[data_full.subject_id == subject_id_test]\n",
    "list_id = list(set(data_train_val.subject_id))\n",
    "list_id.sort()\n",
    "score_dict = {}\n",
    "for subject_id_validate in tqdm(list_id):\n",
    "    data_train = data_train_val[data_train_val.subject_id != subject_id_validate]\n",
    "    data_validate = data_train_val[data_train_val.subject_id == subject_id_validate]\n",
    "    data_train_0 = data_train[data_train.label == 0]\n",
    "    data_train_1 = data_train[data_train.label == 1]\n",
    "    data_validate_0 = data_validate[data_validate.label == 0]\n",
    "    data_validate_1 = data_validate[data_validate.label == 1]\n",
    "    \n",
    "    X_train_0 = data_train_0.iloc[:,:-2].to_numpy()\n",
    "    X_train_1 = data_train_1.iloc[:,:-2].to_numpy()\n",
    "    X_validate_0 = data_validate_0.iloc[:,:-2].to_numpy()\n",
    "    X_validate_1 = data_validate_1.iloc[:,:-2].to_numpy()\n",
    "    \n",
    "    score_0 = similar_distribution(X_validate_0, X_train_0)\n",
    "    score_1 = similar_distribution(X_validate_1, X_train_1)\n",
    "    score = score_0*len(X_validate_0) + score_1*len(X_validate_1)\n",
    "    score = score/(len(X_validate_0) + len(X_validate_1))\n",
    "    score_dict[subject_id_validate] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8c8d5da-bbbb-41bd-91d8-2b37aea0f384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'S10': 0.2459866134857221,\n",
       " 'S11': 0.33541790504651153,\n",
       " 'S13': 0.42526189970302175,\n",
       " 'S15': 0.22464156676636743,\n",
       " 'S16': 0.25591138555050685,\n",
       " 'S17': 0.27247231107971837,\n",
       " 'S2': 0.2773977047535787,\n",
       " 'S3': 0.28797789479702246,\n",
       " 'S4': 0.3894865834976222,\n",
       " 'S5': 0.2361966190212268,\n",
       " 'S6': 0.2567577263052203,\n",
       " 'S7': 0.37740772985482113,\n",
       " 'S8': 0.32538538092690333,\n",
       " 'S9': 0.3436536274600524}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_dict # S14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2bb77723-23c1-4f93-9408-100af4a1317d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'S10': 0.24868616390965503,\n",
       " 'S11': 0.3353142124391459,\n",
       " 'S13': 0.43174628676198884,\n",
       " 'S14': 0.4410605015340969,\n",
       " 'S15': 0.22683161384309725,\n",
       " 'S16': 0.25205764070812436,\n",
       " 'S17': 0.27177808844143997,\n",
       " 'S2': 0.2799986889650542,\n",
       " 'S3': 0.2878490482531951,\n",
       " 'S4': 0.3796193591138874,\n",
       " 'S5': 0.244991436602247,\n",
       " 'S6': 0.2555274678107994,\n",
       " 'S7': 0.38101008313383616,\n",
       " 'S8': 0.3172726849709103}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_dict # S9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffb02ef-fad7-4f36-9389-19de7fe42608",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
